{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nxzgrghCx7d"
   },
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6715,
     "status": "ok",
     "timestamp": 1682171821791,
     "user": {
      "displayName": "Bibas Rai",
      "userId": "14840915709265663697"
     },
     "user_tz": -60
    },
    "id": "syukEjceuEco",
    "outputId": "a08215ec-352e-4a9b-f3c3-5ce47c12b666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\bibas\\anaconda3\\lib\\site-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gzxl_7dVCpxN"
   },
   "source": [
    "## Data Aquisition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "dXzhubJGtoL9"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv', sep='\\t',names=['text','l','id'])\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/dev.tsv', sep='\\t',names=['text','l','id'])\n",
    "df3 = pd.read_csv('https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/test.tsv', sep='\\t',names=['text','l','id'])\n",
    "\n",
    "df = pd.concat([df1,df2,df3], axis=0, ignore_index=True)\n",
    "df = df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>Thanks. I was diagnosed with BP 1 after the ho...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54259</th>\n",
       "      <td>Well that makes sense.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>Daddy issues [NAME]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54261</th>\n",
       "      <td>So glad I discovered that subreddit a couple m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54262</th>\n",
       "      <td>Had to watch \"Elmo in Grouchland\" one time too...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   l\n",
       "0      My favourite food is anything I didn't have to...  27\n",
       "1      Now if he does off himself, everyone will thin...  27\n",
       "2                         WHY THE FUCK IS BAYLESS ISOING   2\n",
       "3                            To make her feel threatened  14\n",
       "4                                 Dirty Southern Wankers   3\n",
       "...                                                  ...  ..\n",
       "54258  Thanks. I was diagnosed with BP 1 after the ho...  15\n",
       "54259                             Well that makes sense.   4\n",
       "54260                                Daddy issues [NAME]  27\n",
       "54261  So glad I discovered that subreddit a couple m...   0\n",
       "54262  Had to watch \"Elmo in Grouchland\" one time too...  27\n",
       "\n",
       "[54263 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1682171966399,
     "user": {
      "displayName": "Bibas Rai",
      "userId": "14840915709265663697"
     },
     "user_tz": -60
    },
    "id": "IiBoribvw5za",
    "outputId": "82836ea0-9039-4f8c-feb5-168a4c700cad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54263, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['l'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\AppData\\Local\\Temp\\ipykernel_7712\\2058537554.py:3: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  x = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "classifieds = [0,0,1,1,8,2,10,4,4,5,6,6,6,0,7,8,9,11,2,7,3,12,3,3,10,10,0,13]\n",
    "\n",
    "x = pd.Series() \n",
    "\n",
    "def lis(x):\n",
    "    return x.split(\",\")\n",
    "\n",
    "x=df['l'].apply(lis)\n",
    "\n",
    "def dataP(a):\n",
    "    l= np.zeros((14,), dtype=int)\n",
    "    for i,v in enumerate(a):\n",
    "        l[classifieds[int(v)]] = 1\n",
    "    return l\n",
    "\n",
    "x=x.apply(dataP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "fkSnNVzCXu7W"
   },
   "outputs": [],
   "source": [
    "classes = [0,0,1,1,8,2,10,4,4,5,6,6,6,0,7,8,9,11,2,7,3,12,3,3,10,10,0,13]\n",
    "\n",
    "\n",
    "def dataP(a):\n",
    "    l=a.split(\",\")\n",
    "    for i,v in enumerate(l):\n",
    "        l[i]=classes[int(v)]\n",
    "    return ','.join(map(str, l))\n",
    "\n",
    "df['l']=df['l'].apply(dataP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>Thanks. I was diagnosed with BP 1 after the ho...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54259</th>\n",
       "      <td>Well that makes sense.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>Daddy issues [NAME]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54261</th>\n",
       "      <td>So glad I discovered that subreddit a couple m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54262</th>\n",
       "      <td>Had to watch \"Elmo in Grouchland\" one time too...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   l\n",
       "0      My favourite food is anything I didn't have to...  13\n",
       "1      Now if he does off himself, everyone will thin...  13\n",
       "2                         WHY THE FUCK IS BAYLESS ISOING   1\n",
       "3                            To make her feel threatened   7\n",
       "4                                 Dirty Southern Wankers   1\n",
       "...                                                  ...  ..\n",
       "54258  Thanks. I was diagnosed with BP 1 after the ho...   8\n",
       "54259                             Well that makes sense.   8\n",
       "54260                                Daddy issues [NAME]  13\n",
       "54261  So glad I discovered that subreddit a couple m...   0\n",
       "54262  Had to watch \"Elmo in Grouchland\" one time too...  13\n",
       "\n",
       "[54263 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - admiration  =  10066\n",
      "1 - amusement  =  4705\n",
      "2 - anger  =  3899\n",
      "3 - annoyance  =  3498\n",
      "4 - approval  =  3508\n",
      "5 - caring  =  1583\n",
      "6 - confusion  =  3907\n",
      "7 - curiosity  =  929\n",
      "8 - desire  =  6986\n",
      "9 - disappointment  =  96\n",
      "10 - disapproval  =  3884\n",
      "11 - disgust  =  1785\n",
      "12 - embarrassment  =  142\n",
      "13 - excitement  =  17772\n"
     ]
    }
   ],
   "source": [
    "class_names= [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\",\"disappointment\", \"disapproval\", \n",
    "               \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
    "               \"relief\", \"remorse\",\" sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "count= np.zeros((14,), dtype=int)\n",
    "\n",
    "for i,q in enumerate(x):\n",
    "    for j,p in enumerate(q):\n",
    "        if p == 1:\n",
    "            count[j] +=1\n",
    "\n",
    "for i in range(14):\n",
    "    print(i,\"-\",class_names[i], \" = \", count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,8'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['l'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['l'][7][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Visualization of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54263, 2)\n",
      "text    0\n",
      "l       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54263 entries, 0 to 54262\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    54263 non-null  object\n",
      " 1   l       54263 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 848.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54263</td>\n",
       "      <td>54263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>53994</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>16021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text      l\n",
       "count        54263  54263\n",
       "unique       53994    426\n",
       "top     Thank you.     13\n",
       "freq            15  16021"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13          16021\n",
       "0            6951\n",
       "8            4745\n",
       "1            3074\n",
       "6            2690\n",
       "            ...  \n",
       "1,7,10          1\n",
       "0,10,4,8        1\n",
       "6,6,13          1\n",
       "0,5,2           1\n",
       "10,11,3         1\n",
       "Name: l, Length: 426, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = df['l'].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = df['l'].value_counts().head(15)\n",
    "plt.bar(value_counts.index, value_counts.values, color=['violet', 'indigo', 'blue', 'green', 'yellow', 'orange' ,'red', 'pink','brown','black'])\n",
    "\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bar Plot for top 15 ferquent labels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZzXsd6aC8v2"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "uHD4HXyFvsJj"
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my favourite food is anything i didn't have to...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now if he does off himself, everyone will thin...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>thanks. i was diagnosed with bp 1 after the ho...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54259</th>\n",
       "      <td>well that makes sense.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>daddy issues [name]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54261</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54262</th>\n",
       "      <td>had to watch \"elmo in grouchland\" one time too...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   l\n",
       "0      my favourite food is anything i didn't have to...  13\n",
       "1      now if he does off himself, everyone will thin...  13\n",
       "2                         why the fuck is bayless isoing   1\n",
       "3                            to make her feel threatened   7\n",
       "4                                 dirty southern wankers   1\n",
       "...                                                  ...  ..\n",
       "54258  thanks. i was diagnosed with bp 1 after the ho...   8\n",
       "54259                             well that makes sense.   8\n",
       "54260                                daddy issues [name]  13\n",
       "54261  so glad i discovered that subreddit a couple m...   0\n",
       "54262  had to watch \"elmo in grouchland\" one time too...  13\n",
       "\n",
       "[54263 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "3Nqzx1QMw8UM"
   },
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "m8wPEVbK2kCG"
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "svlTDBCR3dSw"
   },
   "outputs": [],
   "source": [
    "exclude=string.punctuation\n",
    "def remove_punc(txt):\n",
    "    return txt.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "0g-GBaYS3e-2"
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1682172295964,
     "user": {
      "displayName": "Bibas Rai",
      "userId": "14840915709265663697"
     },
     "user_tz": -60
    },
    "id": "Cz7LcartHLhJ",
    "outputId": "4ecdf380-cc6a-4d1d-b47d-4132b87185e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bibas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "YMJS9Q7pB8vE"
   },
   "outputs": [],
   "source": [
    "def remove_stop(txt):\n",
    "    new=[]\n",
    "    for w in txt.split():\n",
    "        if w in stopwords.words('english'):\n",
    "            new.append('')\n",
    "        else:\n",
    "            new.append(w)\n",
    "    x=new[:]\n",
    "    new.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "VzEFV_CjEteA"
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(remove_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favourite food  anything  didnt   cook</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>everyone  think hes   laugh screwing  pe...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck  bayless isoing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make  feel threatened</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>thanks   diagnosed  bp 1   hospitalization  well</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54259</th>\n",
       "      <td>well  makes sense</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54261</th>\n",
       "      <td>glad  discovered  subreddit  couple months ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54262</th>\n",
       "      <td>watch elmo  grouchland one time  many   kids...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   l\n",
       "0                favourite food  anything  didnt   cook   13\n",
       "1            everyone  think hes   laugh screwing  pe...  13\n",
       "2                                   fuck  bayless isoing   1\n",
       "3                                  make  feel threatened   7\n",
       "4                                 dirty southern wankers   1\n",
       "...                                                  ...  ..\n",
       "54258   thanks   diagnosed  bp 1   hospitalization  well   8\n",
       "54259                                  well  makes sense   8\n",
       "54260                                  daddy issues name  13\n",
       "54261   glad  discovered  subreddit  couple months ag...   0\n",
       "54262    watch elmo  grouchland one time  many   kids...  13\n",
       "\n",
       "[54263 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "GsWJsOfvEyqZ"
   },
   "outputs": [],
   "source": [
    "def demoji(txt):\n",
    "    return emoji.demojize(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1682172423863,
     "user": {
      "displayName": "Bibas Rai",
      "userId": "14840915709265663697"
     },
     "user_tz": -60
    },
    "id": "PdpZDYz_JPtP",
    "outputId": "180ad15a-0aeb-4a9e-83f7-f64f358a0a73"
   },
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(demoji) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "z4XymyW_I-c7"
   },
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem(txt):\n",
    "    return \" \".join([ps.stem(word) for word in txt.split()])\n",
    "\n",
    "df['text']=df['text'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favourit food anyth didnt cook</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>everyon think he laugh screw peopl instead act...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck bayless iso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make feel threaten</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirti southern wanker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>omg peyton isnt good enough help us playoff du...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ye heard abt f bomb thank repli hubbi anxious ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>need board creat bit space name weâ€™ll good</td>\n",
       "      <td>4,3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>damn youtub outrag drama super lucr reddit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>might link trust factor friend</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    l\n",
       "0                     favourit food anyth didnt cook   13\n",
       "1  everyon think he laugh screw peopl instead act...   13\n",
       "2                                   fuck bayless iso    1\n",
       "3                                 make feel threaten    7\n",
       "4                              dirti southern wanker    1\n",
       "5  omg peyton isnt good enough help us playoff du...    0\n",
       "6  ye heard abt f bomb thank repli hubbi anxious ...    8\n",
       "7         need board creat bit space name weâ€™ll good  4,3\n",
       "8         damn youtub outrag drama super lucr reddit    0\n",
       "9                     might link trust factor friend   13"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp. 1 - Model comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['text']\n",
    "y=df['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the total number of Training Data : (32557,)\n",
      "\n",
      " the total number of Test Data : (21706,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "print ('\\n the total number of Training Data :',y_train.shape)\n",
    "print ('\\n the total number of Test Data :',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the total number of Training Data : (32557, 30)\n",
      "\n",
      " the total number of Test Data : (21706, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "print ('\\n the total number of Training Data :',X_train_transformed.shape)\n",
    "print ('\\n the total number of Test Data :',X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37538007924076294\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.19      0.27      2713\n",
      "         0,0       0.00      0.00      0.00       104\n",
      "       0,0,0       0.00      0.00      0.00         3\n",
      "      0,0,11       0.00      0.00      0.00         6\n",
      "      0,0,13       0.00      0.00      0.00         1\n",
      "       0,0,2       0.00      0.00      0.00         2\n",
      "       0,0,3       0.00      0.00      0.00         4\n",
      "       0,0,5       0.00      0.00      0.00         1\n",
      "       0,0,8       0.00      0.00      0.00         3\n",
      "         0,1       0.00      0.00      0.00        38\n",
      "       0,1,1       0.00      0.00      0.00         1\n",
      "    0,1,2,11       0.00      0.00      0.00         1\n",
      "       0,1,3       0.00      0.00      0.00         1\n",
      "       0,1,6       0.00      0.00      0.00         2\n",
      "       0,1,8       0.00      0.00      0.00         1\n",
      "        0,10       0.00      0.00      0.00        40\n",
      "     0,10,11       0.00      0.00      0.00         1\n",
      "   0,10,4,10       0.00      0.00      0.00         1\n",
      "   0,10,4,11       0.00      0.00      0.00         1\n",
      "    0,10,4,8       0.00      0.00      0.00         1\n",
      "      0,10,6       0.00      0.00      0.00         1\n",
      "      0,10,8       0.00      0.00      0.00         2\n",
      "        0,11       0.00      0.00      0.00       109\n",
      "      0,11,0       0.00      0.00      0.00         1\n",
      "     0,11,12       0.00      0.00      0.00         2\n",
      "     0,11,13       0.00      0.00      0.00         1\n",
      "      0,11,2       0.00      0.00      0.00         3\n",
      "    0,11,2,3       0.00      0.00      0.00         1\n",
      "      0,11,3       0.00      0.00      0.00         1\n",
      "        0,12       0.00      0.00      0.00        12\n",
      "        0,13       0.00      0.00      0.00        83\n",
      "         0,2       0.00      0.00      0.00       126\n",
      "      0,2,10       0.00      0.00      0.00         1\n",
      "      0,2,13       0.00      0.00      0.00         1\n",
      "       0,2,2       0.00      0.00      0.00         1\n",
      "       0,2,3       0.00      0.00      0.00         4\n",
      "         0,3       0.00      0.00      0.00        96\n",
      "       0,3,0       0.00      0.00      0.00         1\n",
      "      0,3,10       0.00      0.00      0.00         1\n",
      "      0,3,12       0.00      0.00      0.00         1\n",
      "      0,3,13       0.00      0.00      0.00         1\n",
      "         0,4       0.00      0.00      0.00        53\n",
      "       0,4,0       0.00      0.00      0.00         1\n",
      "      0,4,10       0.00      0.00      0.00         1\n",
      "      0,4,11       0.00      0.00      0.00         2\n",
      "       0,4,2       0.00      0.00      0.00         3\n",
      "       0,4,3       0.00      0.00      0.00         5\n",
      "       0,4,8       0.00      0.00      0.00         2\n",
      "         0,5       0.00      0.00      0.00        13\n",
      "       0,5,3       0.00      0.00      0.00         1\n",
      "       0,5,6       0.00      0.00      0.00         1\n",
      "       0,5,8       0.00      0.00      0.00         1\n",
      "         0,6       0.00      0.00      0.00        33\n",
      "       0,6,0       0.00      0.00      0.00         1\n",
      "     0,6,0,0       0.00      0.00      0.00         1\n",
      "       0,6,2       0.00      0.00      0.00         2\n",
      "       0,6,3       0.00      0.00      0.00         4\n",
      "       0,6,7       0.00      0.00      0.00         1\n",
      "       0,6,8       0.00      0.00      0.00         2\n",
      "     0,6,8,3       0.00      0.00      0.00         1\n",
      "         0,7       0.00      0.00      0.00         8\n",
      "       0,7,0       0.00      0.00      0.00         1\n",
      "      0,7,10       0.00      0.00      0.00         1\n",
      "         0,8       0.00      0.00      0.00       264\n",
      "       0,8,0       0.00      0.00      0.00         5\n",
      "    0,8,0,11       0.00      0.00      0.00         1\n",
      "      0,8,10       0.00      0.00      0.00         1\n",
      "      0,8,11       0.00      0.00      0.00        14\n",
      "    0,8,11,3       0.00      0.00      0.00         1\n",
      "       0,8,2       0.00      0.00      0.00         9\n",
      "       0,8,3       0.00      0.00      0.00        16\n",
      "       0,8,4       0.00      0.00      0.00         1\n",
      "     0,8,4,0       0.00      0.00      0.00         1\n",
      "       0,8,5       0.00      0.00      0.00         2\n",
      "       0,8,7       0.00      0.00      0.00         1\n",
      "       0,8,8       0.00      0.00      0.00         3\n",
      "      0,9,10       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00      1212\n",
      "         1,0       0.00      0.00      0.00         5\n",
      "         1,1       0.00      0.00      0.00       124\n",
      "      1,1,13       0.00      0.00      0.00         1\n",
      "       1,1,5       0.00      0.00      0.00         6\n",
      "       1,1,6       0.00      0.00      0.00         7\n",
      "     1,1,6,6       0.00      0.00      0.00         1\n",
      "       1,1,8       0.00      0.00      0.00         1\n",
      "        1,10       0.00      0.00      0.00        31\n",
      "      1,10,3       0.00      0.00      0.00         1\n",
      "      1,10,6       0.00      0.00      0.00         1\n",
      "      1,10,7       0.00      0.00      0.00         1\n",
      "        1,11       0.00      0.00      0.00         2\n",
      "        1,12       0.00      0.00      0.00         1\n",
      "        1,13       0.00      0.00      0.00        98\n",
      "         1,2       0.00      0.00      0.00        18\n",
      "      1,2,13       0.00      0.00      0.00         1\n",
      "       1,2,3       0.00      0.00      0.00         1\n",
      "         1,3       0.00      0.00      0.00        31\n",
      "         1,4       0.00      0.00      0.00        32\n",
      "       1,4,3       0.00      0.00      0.00         2\n",
      "       1,4,6       0.00      0.00      0.00         1\n",
      "         1,5       0.00      0.00      0.00        56\n",
      "      1,5,10       0.00      0.00      0.00         4\n",
      "       1,5,6       0.00      0.00      0.00         7\n",
      "       1,5,7       0.00      0.00      0.00         1\n",
      "       1,5,9       0.00      0.00      0.00         1\n",
      "         1,6       0.00      0.00      0.00       144\n",
      "      1,6,10       0.00      0.00      0.00         1\n",
      "      1,6,13       0.00      0.00      0.00         1\n",
      "       1,6,6       0.00      0.00      0.00         7\n",
      "       1,6,7       0.00      0.00      0.00         1\n",
      "       1,6,8       0.00      0.00      0.00         1\n",
      "         1,7       0.00      0.00      0.00         8\n",
      "         1,8       0.00      0.00      0.00        34\n",
      "       1,8,2       0.00      0.00      0.00         1\n",
      "       1,8,3       0.00      0.00      0.00         2\n",
      "       1,8,4       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00       993\n",
      "        10,0       0.00      0.00      0.00        15\n",
      "       10,10       0.00      0.00      0.00        19\n",
      "       10,13       0.00      0.00      0.00        66\n",
      "        10,2       0.00      0.00      0.00         3\n",
      "     10,2,10       0.00      0.00      0.00         1\n",
      "      10,2,3       0.00      0.00      0.00         1\n",
      "        10,3       0.00      0.00      0.00        10\n",
      "     10,3,13       0.00      0.00      0.00         1\n",
      "        10,4       0.00      0.00      0.00        79\n",
      "      10,4,0       0.00      0.00      0.00         1\n",
      "     10,4,13       0.00      0.00      0.00         6\n",
      "      10,4,3       0.00      0.00      0.00         2\n",
      "      10,4,5       0.00      0.00      0.00         3\n",
      "    10,4,5,0       0.00      0.00      0.00         1\n",
      "      10,4,6       0.00      0.00      0.00         1\n",
      "      10,4,7       0.00      0.00      0.00         2\n",
      "      10,4,8       0.00      0.00      0.00         1\n",
      "        10,5       0.00      0.00      0.00         5\n",
      "        10,6       0.00      0.00      0.00         9\n",
      "     10,6,10       0.00      0.00      0.00         1\n",
      "     10,6,13       0.00      0.00      0.00         1\n",
      "        10,7       0.00      0.00      0.00         1\n",
      "      10,7,0       0.00      0.00      0.00         1\n",
      "        10,8       0.00      0.00      0.00         8\n",
      "      10,8,0       0.00      0.00      0.00         1\n",
      "      10,8,3       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00       422\n",
      "        11,0       0.00      0.00      0.00         4\n",
      "       11,10       0.00      0.00      0.00         5\n",
      "       11,12       0.00      0.00      0.00         3\n",
      "       11,13       0.00      0.00      0.00        17\n",
      "        11,2       0.00      0.00      0.00        26\n",
      "        11,3       0.00      0.00      0.00        18\n",
      "          12       0.00      0.00      0.00        32\n",
      "       12,13       0.00      0.00      0.00         1\n",
      "          13       0.34      0.98      0.50      6386\n",
      "           2       0.58      0.40      0.48      1056\n",
      "         2,0       0.00      0.00      0.00         3\n",
      "        2,10       0.00      0.00      0.00        19\n",
      "     2,10,10       0.00      0.00      0.00         1\n",
      "        2,11       0.00      0.00      0.00        10\n",
      "      2,11,3       0.00      0.00      0.00         1\n",
      "        2,13       0.00      0.00      0.00        41\n",
      "         2,2       0.00      0.00      0.00        12\n",
      "         2,3       0.00      0.00      0.00        54\n",
      "      2,3,10       0.00      0.00      0.00         2\n",
      "       2,3,3       0.00      0.00      0.00         1\n",
      "         2,4       0.00      0.00      0.00         6\n",
      "       2,4,3       0.00      0.00      0.00         2\n",
      "       2,4,6       0.00      0.00      0.00         1\n",
      "         2,5       0.00      0.00      0.00         2\n",
      "      2,5,10       0.00      0.00      0.00         3\n",
      "       2,5,3       0.00      0.00      0.00         1\n",
      "         2,6       0.00      0.00      0.00         7\n",
      "      2,6,11       0.00      0.00      0.00         1\n",
      "         2,7       0.00      0.00      0.00         6\n",
      "         2,8       0.00      0.00      0.00         7\n",
      "       2,8,3       0.00      0.00      0.00         1\n",
      "         2,9       0.00      0.00      0.00         2\n",
      "      2,9,10       0.00      0.00      0.00         1\n",
      "           3       0.54      0.24      0.33       753\n",
      "         3,0       0.00      0.00      0.00        18\n",
      "        3,10       0.00      0.00      0.00        25\n",
      "        3,12       0.00      0.00      0.00         2\n",
      "        3,13       0.00      0.00      0.00        75\n",
      "         3,3       0.00      0.00      0.00        16\n",
      "      3,3,10       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00       896\n",
      "         4,0       0.00      0.00      0.00        34\n",
      "       4,0,0       0.00      0.00      0.00         1\n",
      "      4,0,11       0.00      0.00      0.00         3\n",
      "       4,0,3       0.00      0.00      0.00         2\n",
      "        4,10       0.00      0.00      0.00        18\n",
      "        4,11       0.00      0.00      0.00         6\n",
      "      4,11,2       0.00      0.00      0.00         1\n",
      "        4,13       0.00      0.00      0.00        70\n",
      "         4,2       0.00      0.00      0.00        24\n",
      "       4,2,0       0.00      0.00      0.00         1\n",
      "      4,2,10       0.00      0.00      0.00         1\n",
      "       4,2,3       0.00      0.00      0.00         1\n",
      "         4,3       0.00      0.00      0.00        53\n",
      "       4,3,3       0.00      0.00      0.00         1\n",
      "         4,4       0.00      0.00      0.00         6\n",
      "       4,4,0       0.00      0.00      0.00         1\n",
      "         4,5       0.00      0.00      0.00         8\n",
      "      4,5,10       0.00      0.00      0.00         2\n",
      "   4,5,10,10       0.00      0.00      0.00         1\n",
      "       4,5,3       0.00      0.00      0.00         1\n",
      "         4,6       0.00      0.00      0.00        25\n",
      "      4,6,10       0.00      0.00      0.00         3\n",
      "      4,6,11       0.00      0.00      0.00         1\n",
      "       4,6,2       0.00      0.00      0.00         1\n",
      "       4,6,3       0.00      0.00      0.00         1\n",
      "       4,6,6       0.00      0.00      0.00         1\n",
      "       4,6,8       0.00      0.00      0.00         1\n",
      "         4,7       0.00      0.00      0.00         5\n",
      "      4,7,10       0.00      0.00      0.00         1\n",
      "         4,8       0.00      0.00      0.00        15\n",
      "       4,8,3       0.00      0.00      0.00         2\n",
      "         4,9       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00       366\n",
      "         5,0       0.00      0.00      0.00         5\n",
      "        5,10       0.00      0.00      0.00        68\n",
      "     5,10,10       0.00      0.00      0.00         1\n",
      "     5,10,13       0.00      0.00      0.00         2\n",
      "        5,11       0.00      0.00      0.00         2\n",
      "   5,11,2,10       0.00      0.00      0.00         1\n",
      "        5,13       0.00      0.00      0.00        35\n",
      "         5,2       0.00      0.00      0.00         5\n",
      "         5,3       0.00      0.00      0.00        17\n",
      "      5,3,10       0.00      0.00      0.00         2\n",
      "         5,6       0.00      0.00      0.00        29\n",
      "      5,6,10       0.00      0.00      0.00         1\n",
      "  5,6,7,7,10       0.00      0.00      0.00         1\n",
      "         5,7       0.00      0.00      0.00         3\n",
      "         5,8       0.00      0.00      0.00         4\n",
      "         5,9       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00      1067\n",
      "         6,0       0.00      0.00      0.00         7\n",
      "      6,0,11       0.00      0.00      0.00         1\n",
      "        6,10       0.00      0.00      0.00        24\n",
      "        6,13       0.00      0.00      0.00        78\n",
      "         6,2       0.00      0.00      0.00         7\n",
      "         6,3       0.00      0.00      0.00        36\n",
      "      6,3,10       0.00      0.00      0.00         2\n",
      "      6,3,12       0.00      0.00      0.00         1\n",
      "         6,6       0.00      0.00      0.00        23\n",
      "      6,6,13       0.00      0.00      0.00         1\n",
      "         6,7       0.00      0.00      0.00         5\n",
      "       6,7,3       0.00      0.00      0.00         2\n",
      "         6,8       0.00      0.00      0.00         7\n",
      "         6,9       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       257\n",
      "         7,0       0.00      0.00      0.00         1\n",
      "        7,10       0.00      0.00      0.00         8\n",
      "        7,11       0.00      0.00      0.00         1\n",
      "      7,11,7       0.00      0.00      0.00         1\n",
      "        7,13       0.00      0.00      0.00        14\n",
      "         7,2       0.00      0.00      0.00         2\n",
      "         7,3       0.00      0.00      0.00         6\n",
      "      7,3,10       0.00      0.00      0.00         1\n",
      "         7,7       0.00      0.00      0.00        12\n",
      "      7,7,13       0.00      0.00      0.00         1\n",
      "      7,8,10       0.00      0.00      0.00         1\n",
      "           8       0.70      0.42      0.52      1910\n",
      "         8,0       0.00      0.00      0.00        30\n",
      "       8,0,0       0.00      0.00      0.00         1\n",
      "      8,0,13       0.00      0.00      0.00         2\n",
      "       8,0,3       0.00      0.00      0.00         1\n",
      "        8,10       0.00      0.00      0.00        35\n",
      "      8,10,4       0.00      0.00      0.00         1\n",
      "        8,11       0.00      0.00      0.00        36\n",
      "      8,11,3       0.00      0.00      0.00         1\n",
      "        8,12       0.00      0.00      0.00         6\n",
      "        8,13       0.00      0.00      0.00       103\n",
      "         8,2       0.00      0.00      0.00        78\n",
      "       8,2,2       0.00      0.00      0.00         1\n",
      "       8,2,3       0.00      0.00      0.00         2\n",
      "       8,2,4       0.00      0.00      0.00         1\n",
      "         8,3       0.00      0.00      0.00       103\n",
      "       8,3,0       0.00      0.00      0.00         1\n",
      "      8,3,10       0.00      0.00      0.00         1\n",
      "      8,3,12       0.00      0.00      0.00         1\n",
      "      8,3,13       0.00      0.00      0.00         2\n",
      "       8,3,3       0.00      0.00      0.00         1\n",
      "         8,4       0.00      0.00      0.00        22\n",
      "       8,4,2       0.00      0.00      0.00         1\n",
      "       8,4,3       0.00      0.00      0.00         3\n",
      "         8,5       0.00      0.00      0.00         8\n",
      "   8,5,10,13       0.00      0.00      0.00         1\n",
      "       8,5,3       0.00      0.00      0.00         2\n",
      "         8,6       0.00      0.00      0.00        15\n",
      "         8,7       0.00      0.00      0.00         6\n",
      "       8,7,2       0.00      0.00      0.00         1\n",
      "         8,8       0.00      0.00      0.00        32\n",
      "      8,9,10       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00        25\n",
      "         9,0       0.00      0.00      0.00         1\n",
      "        9,10       0.00      0.00      0.00        13\n",
      "     9,10,10       0.00      0.00      0.00         1\n",
      "        9,13       0.00      0.00      0.00         1\n",
      "         9,2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.38     21706\n",
      "   macro avg       0.01      0.01      0.01     21706\n",
      "weighted avg       0.27      0.38      0.26     21706\n",
      "\n",
      "Accuracy: 0.37538007924076294\n",
      "Precision: 0.37538007924076294\n",
      "Recall: 0.37538007924076294\n",
      "F1 score: 0.37538007924076294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision=precision_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "classes = np.unique(y_test)\n",
    "report = classification_report(y_test, y_pred, labels=classes)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "print(\"Classification report:\\n\", report)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y=df['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the total number of Training Data : (32557,)\n",
      "\n",
      " the total number of Test Data : (21706,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "print ('\\n the total number of Training Data :',y_train.shape)\n",
    "print ('\\n the total number of Test Data :',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=30)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4929051875057588\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.68      0.60      2713\n",
      "         0,0       0.00      0.00      0.00       104\n",
      "       0,0,0       0.00      0.00      0.00         3\n",
      "      0,0,11       0.00      0.00      0.00         6\n",
      "      0,0,13       0.00      0.00      0.00         1\n",
      "       0,0,2       0.00      0.00      0.00         2\n",
      "       0,0,3       0.00      0.00      0.00         4\n",
      "       0,0,5       0.00      0.00      0.00         1\n",
      "       0,0,8       0.00      0.00      0.00         3\n",
      "         0,1       0.00      0.00      0.00        38\n",
      "       0,1,1       0.00      0.00      0.00         1\n",
      "    0,1,2,11       0.00      0.00      0.00         1\n",
      "       0,1,3       0.00      0.00      0.00         1\n",
      "       0,1,6       0.00      0.00      0.00         2\n",
      "       0,1,8       0.00      0.00      0.00         1\n",
      "        0,10       0.00      0.00      0.00        40\n",
      "     0,10,11       0.00      0.00      0.00         1\n",
      "   0,10,4,10       0.00      0.00      0.00         1\n",
      "   0,10,4,11       0.00      0.00      0.00         1\n",
      "    0,10,4,8       0.00      0.00      0.00         1\n",
      "      0,10,6       0.00      0.00      0.00         1\n",
      "      0,10,8       0.00      0.00      0.00         2\n",
      "        0,11       1.00      0.01      0.02       109\n",
      "      0,11,0       0.00      0.00      0.00         1\n",
      "     0,11,12       0.00      0.00      0.00         2\n",
      "     0,11,13       0.00      0.00      0.00         1\n",
      "      0,11,2       0.00      0.00      0.00         3\n",
      "    0,11,2,3       0.00      0.00      0.00         1\n",
      "      0,11,3       0.00      0.00      0.00         1\n",
      "        0,12       0.00      0.00      0.00        12\n",
      "        0,13       0.00      0.00      0.00        83\n",
      "         0,2       0.25      0.03      0.06       126\n",
      "      0,2,10       0.00      0.00      0.00         1\n",
      "      0,2,13       0.00      0.00      0.00         1\n",
      "       0,2,2       0.00      0.00      0.00         1\n",
      "       0,2,3       0.00      0.00      0.00         4\n",
      "         0,3       0.33      0.02      0.04        96\n",
      "       0,3,0       0.00      0.00      0.00         1\n",
      "      0,3,10       0.00      0.00      0.00         1\n",
      "      0,3,12       0.00      0.00      0.00         1\n",
      "      0,3,13       0.00      0.00      0.00         1\n",
      "         0,4       0.00      0.00      0.00        53\n",
      "       0,4,0       0.00      0.00      0.00         1\n",
      "      0,4,10       0.00      0.00      0.00         1\n",
      "      0,4,11       0.00      0.00      0.00         2\n",
      "       0,4,2       0.00      0.00      0.00         3\n",
      "       0,4,3       0.00      0.00      0.00         5\n",
      "       0,4,8       0.00      0.00      0.00         2\n",
      "         0,5       0.00      0.00      0.00        13\n",
      "       0,5,3       0.00      0.00      0.00         1\n",
      "       0,5,6       0.00      0.00      0.00         1\n",
      "       0,5,8       0.00      0.00      0.00         1\n",
      "         0,6       0.00      0.00      0.00        33\n",
      "       0,6,0       0.00      0.00      0.00         1\n",
      "     0,6,0,0       0.00      0.00      0.00         1\n",
      "       0,6,2       0.00      0.00      0.00         2\n",
      "       0,6,3       0.00      0.00      0.00         4\n",
      "       0,6,7       0.00      0.00      0.00         1\n",
      "       0,6,8       0.00      0.00      0.00         2\n",
      "     0,6,8,3       0.00      0.00      0.00         1\n",
      "         0,7       0.00      0.00      0.00         8\n",
      "       0,7,0       0.00      0.00      0.00         1\n",
      "      0,7,10       0.00      0.00      0.00         1\n",
      "         0,8       0.56      0.12      0.20       264\n",
      "       0,8,0       0.00      0.00      0.00         5\n",
      "    0,8,0,11       0.00      0.00      0.00         1\n",
      "      0,8,10       0.00      0.00      0.00         1\n",
      "      0,8,11       0.00      0.00      0.00        14\n",
      "    0,8,11,3       0.00      0.00      0.00         1\n",
      "       0,8,2       0.00      0.00      0.00         9\n",
      "       0,8,3       0.00      0.00      0.00        16\n",
      "       0,8,4       0.00      0.00      0.00         1\n",
      "     0,8,4,0       0.00      0.00      0.00         1\n",
      "       0,8,5       0.00      0.00      0.00         2\n",
      "       0,8,7       0.00      0.00      0.00         1\n",
      "       0,8,8       0.00      0.00      0.00         3\n",
      "      0,9,10       0.00      0.00      0.00         1\n",
      "           1       0.45      0.35      0.40      1212\n",
      "         1,0       0.00      0.00      0.00         5\n",
      "         1,1       0.00      0.00      0.00       124\n",
      "      1,1,13       0.00      0.00      0.00         1\n",
      "       1,1,5       0.00      0.00      0.00         6\n",
      "       1,1,6       0.00      0.00      0.00         7\n",
      "     1,1,6,6       0.00      0.00      0.00         1\n",
      "       1,1,8       0.00      0.00      0.00         1\n",
      "        1,10       0.00      0.00      0.00        31\n",
      "      1,10,3       0.00      0.00      0.00         1\n",
      "      1,10,6       0.00      0.00      0.00         1\n",
      "      1,10,7       0.00      0.00      0.00         1\n",
      "        1,11       0.00      0.00      0.00         2\n",
      "        1,12       0.00      0.00      0.00         1\n",
      "        1,13       0.00      0.00      0.00        98\n",
      "         1,2       0.00      0.00      0.00        18\n",
      "      1,2,13       0.00      0.00      0.00         1\n",
      "       1,2,3       0.00      0.00      0.00         1\n",
      "         1,3       0.00      0.00      0.00        31\n",
      "         1,4       0.00      0.00      0.00        32\n",
      "       1,4,3       0.00      0.00      0.00         2\n",
      "       1,4,6       0.00      0.00      0.00         1\n",
      "         1,5       0.00      0.00      0.00        56\n",
      "      1,5,10       0.00      0.00      0.00         4\n",
      "       1,5,6       0.00      0.00      0.00         7\n",
      "       1,5,7       0.00      0.00      0.00         1\n",
      "       1,5,9       0.00      0.00      0.00         1\n",
      "         1,6       0.00      0.00      0.00       144\n",
      "      1,6,10       0.00      0.00      0.00         1\n",
      "      1,6,13       0.00      0.00      0.00         1\n",
      "       1,6,6       0.00      0.00      0.00         7\n",
      "       1,6,7       0.00      0.00      0.00         1\n",
      "       1,6,8       0.00      0.00      0.00         1\n",
      "         1,7       0.00      0.00      0.00         8\n",
      "         1,8       0.50      0.09      0.15        34\n",
      "       1,8,2       0.00      0.00      0.00         1\n",
      "       1,8,3       0.00      0.00      0.00         2\n",
      "       1,8,4       0.00      0.00      0.00         1\n",
      "          10       0.54      0.36      0.43       993\n",
      "        10,0       0.00      0.00      0.00        15\n",
      "       10,10       0.00      0.00      0.00        19\n",
      "       10,13       0.00      0.00      0.00        66\n",
      "        10,2       0.00      0.00      0.00         3\n",
      "     10,2,10       0.00      0.00      0.00         1\n",
      "      10,2,3       0.00      0.00      0.00         1\n",
      "        10,3       0.00      0.00      0.00        10\n",
      "     10,3,13       0.00      0.00      0.00         1\n",
      "        10,4       0.00      0.00      0.00        79\n",
      "      10,4,0       0.00      0.00      0.00         1\n",
      "     10,4,13       0.00      0.00      0.00         6\n",
      "      10,4,3       0.00      0.00      0.00         2\n",
      "      10,4,5       0.00      0.00      0.00         3\n",
      "    10,4,5,0       0.00      0.00      0.00         1\n",
      "      10,4,6       0.00      0.00      0.00         1\n",
      "      10,4,7       0.00      0.00      0.00         2\n",
      "      10,4,8       0.00      0.00      0.00         1\n",
      "        10,5       0.00      0.00      0.00         5\n",
      "        10,6       0.00      0.00      0.00         9\n",
      "     10,6,10       0.00      0.00      0.00         1\n",
      "     10,6,13       0.00      0.00      0.00         1\n",
      "        10,7       0.00      0.00      0.00         1\n",
      "      10,7,0       0.00      0.00      0.00         1\n",
      "        10,8       0.00      0.00      0.00         8\n",
      "      10,8,0       0.00      0.00      0.00         1\n",
      "      10,8,3       0.00      0.00      0.00         1\n",
      "          11       0.41      0.52      0.46       422\n",
      "        11,0       0.00      0.00      0.00         4\n",
      "       11,10       0.00      0.00      0.00         5\n",
      "       11,12       0.00      0.00      0.00         3\n",
      "       11,13       0.00      0.00      0.00        17\n",
      "        11,2       0.00      0.00      0.00        26\n",
      "        11,3       0.00      0.00      0.00        18\n",
      "          12       0.22      0.06      0.10        32\n",
      "       12,13       0.00      0.00      0.00         1\n",
      "          13       0.45      0.87      0.60      6386\n",
      "           2       0.55      0.59      0.57      1056\n",
      "         2,0       0.00      0.00      0.00         3\n",
      "        2,10       0.00      0.00      0.00        19\n",
      "     2,10,10       0.00      0.00      0.00         1\n",
      "        2,11       0.00      0.00      0.00        10\n",
      "      2,11,3       0.00      0.00      0.00         1\n",
      "        2,13       0.00      0.00      0.00        41\n",
      "         2,2       0.00      0.00      0.00        12\n",
      "         2,3       0.00      0.00      0.00        54\n",
      "      2,3,10       0.00      0.00      0.00         2\n",
      "       2,3,3       0.00      0.00      0.00         1\n",
      "         2,4       0.00      0.00      0.00         6\n",
      "       2,4,3       0.00      0.00      0.00         2\n",
      "       2,4,6       0.00      0.00      0.00         1\n",
      "         2,5       0.00      0.00      0.00         2\n",
      "      2,5,10       0.00      0.00      0.00         3\n",
      "       2,5,3       0.00      0.00      0.00         1\n",
      "         2,6       0.00      0.00      0.00         7\n",
      "      2,6,11       0.00      0.00      0.00         1\n",
      "         2,7       0.00      0.00      0.00         6\n",
      "         2,8       0.00      0.00      0.00         7\n",
      "       2,8,3       0.00      0.00      0.00         1\n",
      "         2,9       0.00      0.00      0.00         2\n",
      "      2,9,10       0.00      0.00      0.00         1\n",
      "           3       0.49      0.35      0.41       753\n",
      "         3,0       0.00      0.00      0.00        18\n",
      "        3,10       0.00      0.00      0.00        25\n",
      "        3,12       0.00      0.00      0.00         2\n",
      "        3,13       0.00      0.00      0.00        75\n",
      "         3,3       0.00      0.00      0.00        16\n",
      "      3,3,10       0.00      0.00      0.00         1\n",
      "           4       0.62      0.15      0.24       896\n",
      "         4,0       0.00      0.00      0.00        34\n",
      "       4,0,0       0.00      0.00      0.00         1\n",
      "      4,0,11       0.00      0.00      0.00         3\n",
      "       4,0,3       0.00      0.00      0.00         2\n",
      "        4,10       0.00      0.00      0.00        18\n",
      "        4,11       0.00      0.00      0.00         6\n",
      "      4,11,2       0.00      0.00      0.00         1\n",
      "        4,13       0.00      0.00      0.00        70\n",
      "         4,2       0.00      0.00      0.00        24\n",
      "       4,2,0       0.00      0.00      0.00         1\n",
      "      4,2,10       0.00      0.00      0.00         1\n",
      "       4,2,3       0.00      0.00      0.00         1\n",
      "         4,3       0.00      0.00      0.00        53\n",
      "       4,3,3       0.00      0.00      0.00         1\n",
      "         4,4       0.00      0.00      0.00         6\n",
      "       4,4,0       0.00      0.00      0.00         1\n",
      "         4,5       0.00      0.00      0.00         8\n",
      "      4,5,10       0.00      0.00      0.00         2\n",
      "   4,5,10,10       0.00      0.00      0.00         1\n",
      "       4,5,3       0.00      0.00      0.00         1\n",
      "         4,6       0.00      0.00      0.00        25\n",
      "      4,6,10       0.00      0.00      0.00         3\n",
      "      4,6,11       0.00      0.00      0.00         1\n",
      "       4,6,2       0.00      0.00      0.00         1\n",
      "       4,6,3       0.00      0.00      0.00         1\n",
      "       4,6,6       0.00      0.00      0.00         1\n",
      "       4,6,8       0.00      0.00      0.00         1\n",
      "         4,7       0.00      0.00      0.00         5\n",
      "      4,7,10       0.00      0.00      0.00         1\n",
      "         4,8       0.00      0.00      0.00        15\n",
      "       4,8,3       0.00      0.00      0.00         2\n",
      "         4,9       0.00      0.00      0.00         1\n",
      "           5       0.47      0.09      0.15       366\n",
      "         5,0       0.00      0.00      0.00         5\n",
      "        5,10       0.00      0.00      0.00        68\n",
      "     5,10,10       0.00      0.00      0.00         1\n",
      "     5,10,13       0.00      0.00      0.00         2\n",
      "        5,11       0.00      0.00      0.00         2\n",
      "   5,11,2,10       0.00      0.00      0.00         1\n",
      "        5,13       0.00      0.00      0.00        35\n",
      "         5,2       0.00      0.00      0.00         5\n",
      "         5,3       0.00      0.00      0.00        17\n",
      "      5,3,10       0.00      0.00      0.00         2\n",
      "         5,6       0.00      0.00      0.00        29\n",
      "      5,6,10       0.00      0.00      0.00         1\n",
      "  5,6,7,7,10       0.00      0.00      0.00         1\n",
      "         5,7       0.00      0.00      0.00         3\n",
      "         5,8       0.00      0.00      0.00         4\n",
      "         5,9       0.00      0.00      0.00         1\n",
      "           6       0.49      0.15      0.23      1067\n",
      "         6,0       0.00      0.00      0.00         7\n",
      "      6,0,11       0.00      0.00      0.00         1\n",
      "        6,10       0.00      0.00      0.00        24\n",
      "        6,13       0.00      0.00      0.00        78\n",
      "         6,2       0.00      0.00      0.00         7\n",
      "         6,3       0.00      0.00      0.00        36\n",
      "      6,3,10       0.00      0.00      0.00         2\n",
      "      6,3,12       0.00      0.00      0.00         1\n",
      "         6,6       0.00      0.00      0.00        23\n",
      "      6,6,13       0.00      0.00      0.00         1\n",
      "         6,7       0.00      0.00      0.00         5\n",
      "       6,7,3       0.00      0.00      0.00         2\n",
      "         6,8       0.00      0.00      0.00         7\n",
      "         6,9       0.00      0.00      0.00         1\n",
      "           7       0.50      0.48      0.49       257\n",
      "         7,0       0.00      0.00      0.00         1\n",
      "        7,10       0.00      0.00      0.00         8\n",
      "        7,11       0.00      0.00      0.00         1\n",
      "      7,11,7       0.00      0.00      0.00         1\n",
      "        7,13       0.00      0.00      0.00        14\n",
      "         7,2       0.00      0.00      0.00         2\n",
      "         7,3       0.00      0.00      0.00         6\n",
      "      7,3,10       0.00      0.00      0.00         1\n",
      "         7,7       0.00      0.00      0.00        12\n",
      "      7,7,13       0.00      0.00      0.00         1\n",
      "      7,8,10       0.00      0.00      0.00         1\n",
      "           8       0.70      0.46      0.56      1910\n",
      "         8,0       0.00      0.00      0.00        30\n",
      "       8,0,0       0.00      0.00      0.00         1\n",
      "      8,0,13       0.00      0.00      0.00         2\n",
      "       8,0,3       0.00      0.00      0.00         1\n",
      "        8,10       0.00      0.00      0.00        35\n",
      "      8,10,4       0.00      0.00      0.00         1\n",
      "        8,11       0.00      0.00      0.00        36\n",
      "      8,11,3       0.00      0.00      0.00         1\n",
      "        8,12       0.00      0.00      0.00         6\n",
      "        8,13       0.00      0.00      0.00       103\n",
      "         8,2       0.69      0.14      0.23        78\n",
      "       8,2,2       0.00      0.00      0.00         1\n",
      "       8,2,3       0.00      0.00      0.00         2\n",
      "       8,2,4       0.00      0.00      0.00         1\n",
      "         8,3       0.50      0.08      0.13       103\n",
      "       8,3,0       0.00      0.00      0.00         1\n",
      "      8,3,10       0.00      0.00      0.00         1\n",
      "      8,3,12       0.00      0.00      0.00         1\n",
      "      8,3,13       0.00      0.00      0.00         2\n",
      "       8,3,3       0.00      0.00      0.00         1\n",
      "         8,4       0.00      0.00      0.00        22\n",
      "       8,4,2       0.00      0.00      0.00         1\n",
      "       8,4,3       0.00      0.00      0.00         3\n",
      "         8,5       0.00      0.00      0.00         8\n",
      "   8,5,10,13       0.00      0.00      0.00         1\n",
      "       8,5,3       0.00      0.00      0.00         2\n",
      "         8,6       0.00      0.00      0.00        15\n",
      "         8,7       0.00      0.00      0.00         6\n",
      "       8,7,2       0.00      0.00      0.00         1\n",
      "         8,8       0.00      0.00      0.00        32\n",
      "      8,9,10       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00        25\n",
      "         9,0       0.00      0.00      0.00         1\n",
      "        9,10       0.00      0.00      0.00        13\n",
      "     9,10,10       0.00      0.00      0.00         1\n",
      "        9,13       0.00      0.00      0.00         1\n",
      "         9,2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.49     21706\n",
      "   macro avg       0.03      0.02      0.02     21706\n",
      "weighted avg       0.45      0.49      0.43     21706\n",
      "\n",
      "Accuracy: 0.4929051875057588\n",
      "Precision: 0.4929051875057588\n",
      "Recall: 0.4929051875057588\n",
      "F1 score: 0.4929051875057588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision=precision_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "classes = np.unique(y_test)\n",
    "report = classification_report(y_test, y_pred, labels=classes)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "print(\"Classification report:\\n\", report)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 2 - N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4707454160140053\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.68      0.59      1372\n",
      "         0,0       0.00      0.00      0.00        47\n",
      "       0,0,0       0.00      0.00      0.00         2\n",
      "      0,0,11       0.00      0.00      0.00         2\n",
      "      0,0,13       0.00      0.00      0.00         1\n",
      "       0,0,2       0.00      0.00      0.00         1\n",
      "       0,0,3       0.00      0.00      0.00         2\n",
      "       0,0,5       0.00      0.00      0.00         1\n",
      "       0,0,8       0.00      0.00      0.00         2\n",
      "         0,1       0.00      0.00      0.00        21\n",
      "       0,1,1       0.00      0.00      0.00         1\n",
      "    0,1,2,11       0.00      0.00      0.00         1\n",
      "       0,1,8       0.00      0.00      0.00         1\n",
      "        0,10       0.00      0.00      0.00        18\n",
      "     0,10,11       0.00      0.00      0.00         1\n",
      "   0,10,4,11       0.00      0.00      0.00         1\n",
      "    0,10,4,8       0.00      0.00      0.00         1\n",
      "      0,10,8       0.00      0.00      0.00         1\n",
      "        0,11       0.12      0.02      0.03        60\n",
      "      0,11,0       0.00      0.00      0.00         1\n",
      "     0,11,12       0.00      0.00      0.00         1\n",
      "      0,11,2       0.00      0.00      0.00         1\n",
      "    0,11,2,3       0.00      0.00      0.00         1\n",
      "      0,11,3       0.00      0.00      0.00         1\n",
      "        0,12       0.50      0.20      0.29         5\n",
      "        0,13       0.00      0.00      0.00        47\n",
      "         0,2       0.18      0.10      0.13        58\n",
      "       0,2,3       0.00      0.00      0.00         2\n",
      "         0,3       0.12      0.07      0.09        44\n",
      "      0,3,10       0.00      0.00      0.00         1\n",
      "      0,3,12       0.00      0.00      0.00         1\n",
      "         0,4       0.00      0.00      0.00        30\n",
      "      0,4,11       0.00      0.00      0.00         1\n",
      "       0,4,2       0.00      0.00      0.00         2\n",
      "       0,4,3       0.00      0.00      0.00         1\n",
      "       0,4,8       0.00      0.00      0.00         1\n",
      "         0,5       0.00      0.00      0.00         4\n",
      "       0,5,6       0.00      0.00      0.00         1\n",
      "         0,6       0.00      0.00      0.00         9\n",
      "       0,6,0       0.00      0.00      0.00         1\n",
      "       0,6,2       0.00      0.00      0.00         1\n",
      "       0,6,3       0.00      0.00      0.00         3\n",
      "       0,6,7       0.00      0.00      0.00         1\n",
      "       0,6,8       0.00      0.00      0.00         1\n",
      "     0,6,8,3       0.00      0.00      0.00         1\n",
      "         0,7       0.00      0.00      0.00         2\n",
      "         0,8       0.32      0.17      0.22       143\n",
      "       0,8,0       0.00      0.00      0.00         2\n",
      "      0,8,10       0.00      0.00      0.00         1\n",
      "      0,8,11       0.00      0.00      0.00         9\n",
      "       0,8,2       0.00      0.00      0.00         2\n",
      "       0,8,3       0.00      0.00      0.00         7\n",
      "       0,8,5       0.00      0.00      0.00         2\n",
      "       0,8,7       0.00      0.00      0.00         1\n",
      "       0,8,8       0.00      0.00      0.00         2\n",
      "           1       0.41      0.38      0.39       650\n",
      "         1,0       0.00      0.00      0.00         2\n",
      "         1,1       0.13      0.03      0.05        59\n",
      "       1,1,5       0.00      0.00      0.00         3\n",
      "       1,1,6       0.00      0.00      0.00         3\n",
      "        1,10       0.00      0.00      0.00        12\n",
      "      1,10,6       0.00      0.00      0.00         1\n",
      "      1,10,7       0.00      0.00      0.00         1\n",
      "        1,11       0.00      0.00      0.00         2\n",
      "        1,13       0.00      0.00      0.00        55\n",
      "         1,2       0.00      0.00      0.00         5\n",
      "      1,2,13       0.00      0.00      0.00         1\n",
      "         1,3       0.00      0.00      0.00        18\n",
      "         1,4       0.00      0.00      0.00        14\n",
      "       1,4,3       0.00      0.00      0.00         1\n",
      "       1,4,6       0.00      0.00      0.00         1\n",
      "         1,5       0.00      0.00      0.00        29\n",
      "      1,5,10       0.00      0.00      0.00         1\n",
      "       1,5,6       0.00      0.00      0.00         6\n",
      "         1,6       0.17      0.03      0.05        70\n",
      "      1,6,10       0.00      0.00      0.00         1\n",
      "       1,6,6       0.00      0.00      0.00         5\n",
      "       1,6,7       0.00      0.00      0.00         1\n",
      "         1,7       0.00      0.00      0.00         4\n",
      "         1,8       0.67      0.22      0.33        18\n",
      "       1,8,3       0.00      0.00      0.00         2\n",
      "          10       0.41      0.43      0.42       472\n",
      "        10,0       0.00      0.00      0.00         7\n",
      "       10,10       0.00      0.00      0.00        11\n",
      "       10,13       0.00      0.00      0.00        30\n",
      "     10,2,10       0.00      0.00      0.00         1\n",
      "        10,3       0.00      0.00      0.00         5\n",
      "        10,4       0.00      0.00      0.00        35\n",
      "      10,4,0       0.00      0.00      0.00         1\n",
      "     10,4,13       0.00      0.00      0.00         4\n",
      "      10,4,3       0.00      0.00      0.00         2\n",
      "      10,4,5       0.00      0.00      0.00         1\n",
      "        10,5       0.00      0.00      0.00         3\n",
      "        10,6       0.00      0.00      0.00         3\n",
      "     10,6,10       0.00      0.00      0.00         1\n",
      "        10,7       0.00      0.00      0.00         1\n",
      "      10,7,0       0.00      0.00      0.00         1\n",
      "        10,8       0.00      0.00      0.00         2\n",
      "      10,8,3       0.00      0.00      0.00         1\n",
      "          11       0.36      0.43      0.39       207\n",
      "        11,0       0.00      0.00      0.00         1\n",
      "       11,10       0.00      0.00      0.00         3\n",
      "       11,12       0.00      0.00      0.00         2\n",
      "       11,13       0.00      0.00      0.00        10\n",
      "        11,2       0.33      0.06      0.11        16\n",
      "        11,3       0.00      0.00      0.00         9\n",
      "          12       0.18      0.20      0.19        20\n",
      "       12,13       0.00      0.00      0.00         1\n",
      "          13       0.49      0.75      0.59      3175\n",
      "           2       0.49      0.56      0.52       547\n",
      "         2,0       0.00      0.00      0.00         2\n",
      "        2,10       1.00      0.17      0.29         6\n",
      "        2,11       0.00      0.00      0.00         6\n",
      "        2,13       0.00      0.00      0.00        21\n",
      "         2,2       0.00      0.00      0.00         4\n",
      "         2,3       0.20      0.04      0.06        27\n",
      "      2,3,10       0.00      0.00      0.00         1\n",
      "       2,3,3       0.00      0.00      0.00         1\n",
      "         2,4       0.00      0.00      0.00         1\n",
      "       2,4,3       0.00      0.00      0.00         1\n",
      "         2,5       0.00      0.00      0.00         2\n",
      "      2,5,10       0.00      0.00      0.00         1\n",
      "       2,5,3       0.00      0.00      0.00         1\n",
      "         2,6       0.00      0.00      0.00         2\n",
      "      2,6,11       0.00      0.00      0.00         1\n",
      "         2,7       0.00      0.00      0.00         3\n",
      "         2,8       0.00      0.00      0.00         1\n",
      "       2,8,3       0.00      0.00      0.00         1\n",
      "           3       0.42      0.38      0.40       349\n",
      "         3,0       0.00      0.00      0.00        11\n",
      "        3,10       1.00      0.09      0.17        11\n",
      "        3,12       0.00      0.00      0.00         1\n",
      "        3,13       0.00      0.00      0.00        42\n",
      "         3,3       0.00      0.00      0.00         7\n",
      "           4       0.39      0.18      0.24       427\n",
      "         4,0       0.00      0.00      0.00        18\n",
      "      4,0,11       0.00      0.00      0.00         1\n",
      "       4,0,3       0.00      0.00      0.00         2\n",
      "        4,10       0.00      0.00      0.00         9\n",
      "        4,11       0.00      0.00      0.00         3\n",
      "      4,11,2       0.00      0.00      0.00         1\n",
      "        4,13       0.00      0.00      0.00        29\n",
      "         4,2       0.00      0.00      0.00        12\n",
      "      4,2,10       0.00      0.00      0.00         1\n",
      "         4,3       0.00      0.00      0.00        25\n",
      "         4,4       0.00      0.00      0.00         4\n",
      "       4,4,0       0.00      0.00      0.00         1\n",
      "         4,5       0.00      0.00      0.00         4\n",
      "      4,5,10       0.00      0.00      0.00         2\n",
      "   4,5,10,10       0.00      0.00      0.00         1\n",
      "         4,6       0.00      0.00      0.00         9\n",
      "      4,6,10       0.00      0.00      0.00         3\n",
      "       4,6,2       0.00      0.00      0.00         1\n",
      "       4,6,6       0.00      0.00      0.00         1\n",
      "      4,7,10       0.00      0.00      0.00         1\n",
      "         4,8       0.00      0.00      0.00        11\n",
      "       4,8,3       0.00      0.00      0.00         1\n",
      "           5       0.14      0.06      0.09       176\n",
      "         5,0       0.00      0.00      0.00         2\n",
      "        5,10       0.00      0.00      0.00        26\n",
      "     5,10,10       0.00      0.00      0.00         1\n",
      "     5,10,13       0.00      0.00      0.00         1\n",
      "        5,13       0.00      0.00      0.00        24\n",
      "         5,2       0.00      0.00      0.00         2\n",
      "         5,3       0.00      0.00      0.00         8\n",
      "         5,6       0.00      0.00      0.00        15\n",
      "         5,7       0.00      0.00      0.00         3\n",
      "         5,8       0.00      0.00      0.00         1\n",
      "         5,9       0.00      0.00      0.00         1\n",
      "           6       0.35      0.21      0.26       544\n",
      "         6,0       0.00      0.00      0.00         2\n",
      "      6,0,11       0.00      0.00      0.00         1\n",
      "        6,10       0.00      0.00      0.00        14\n",
      "        6,13       0.00      0.00      0.00        38\n",
      "         6,2       0.00      0.00      0.00         3\n",
      "         6,3       0.00      0.00      0.00        21\n",
      "      6,3,10       0.00      0.00      0.00         2\n",
      "      6,3,12       0.00      0.00      0.00         1\n",
      "         6,6       0.00      0.00      0.00        14\n",
      "      6,6,13       0.00      0.00      0.00         1\n",
      "         6,7       0.00      0.00      0.00         1\n",
      "       6,7,3       0.00      0.00      0.00         1\n",
      "         6,8       0.00      0.00      0.00         3\n",
      "         6,9       0.00      0.00      0.00         1\n",
      "           7       0.47      0.50      0.49       142\n",
      "         7,0       0.00      0.00      0.00         1\n",
      "        7,10       0.00      0.00      0.00         6\n",
      "        7,13       0.00      0.00      0.00         7\n",
      "         7,2       0.00      0.00      0.00         1\n",
      "         7,3       0.00      0.00      0.00         1\n",
      "      7,3,10       0.00      0.00      0.00         1\n",
      "         7,7       0.00      0.00      0.00         6\n",
      "      7,8,10       0.00      0.00      0.00         1\n",
      "           8       0.58      0.49      0.53       980\n",
      "         8,0       0.00      0.00      0.00        14\n",
      "       8,0,3       0.00      0.00      0.00         1\n",
      "        8,10       0.50      0.06      0.11        16\n",
      "        8,11       0.33      0.07      0.12        14\n",
      "      8,11,3       0.00      0.00      0.00         1\n",
      "        8,12       0.00      0.00      0.00         4\n",
      "        8,13       0.00      0.00      0.00        45\n",
      "         8,2       0.60      0.19      0.29        48\n",
      "       8,2,3       0.00      0.00      0.00         1\n",
      "         8,3       0.32      0.13      0.18        54\n",
      "      8,3,12       0.00      0.00      0.00         1\n",
      "      8,3,13       0.00      0.00      0.00         1\n",
      "       8,3,3       0.00      0.00      0.00         1\n",
      "         8,4       0.00      0.00      0.00        11\n",
      "         8,5       0.00      0.00      0.00         2\n",
      "         8,6       0.00      0.00      0.00         8\n",
      "         8,7       0.00      0.00      0.00         1\n",
      "         8,8       0.33      0.05      0.09        19\n",
      "      8,9,10       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00        14\n",
      "         9,0       0.00      0.00      0.00         1\n",
      "        9,10       0.00      0.00      0.00         6\n",
      "     9,10,10       0.00      0.00      0.00         1\n",
      "        9,13       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.47      0.47      0.47     10853\n",
      "   macro avg       0.06      0.03      0.04     10853\n",
      "weighted avg       0.41      0.47      0.42     10853\n",
      "\n",
      "Accuracy: 0.4707454160140053\n",
      "Precision: 0.4707454160140053\n",
      "Recall: 0.4707454160140053\n",
      "F1 score: 0.4707454160140053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision=precision_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "classes = np.unique(y_test)\n",
    "report = classification_report(y_test, y_pred, labels=classes)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "print(\"Classification report:\\n\", report)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 3 - Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the total number of Training Data : (43410,)\n",
      "\n",
      " the total number of Test Data : (10853,)\n",
      "\n",
      " the total number of Training Data : (43410, 10)\n",
      "\n",
      " the total number of Test Data : (10853, 10)\n",
      "Accuracy: 0.33603611904542524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bibas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1396: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x=df['text']\n",
    "y=df['l']\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print ('\\n the total number of Training Data :',y_train.shape)\n",
    "print ('\\n the total number of Test Data :',y_test.shape)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "print ('\\n the total number of Training Data :',X_train_transformed.shape)\n",
    "print ('\\n the total number of Test Data :',X_test_transformed.shape)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_transformed)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision=precision_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "classes = np.unique(y_test)\n",
    "report = classification_report(y_test, y_pred, labels=classes)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.08      0.12      1372\n",
      "         0,0       0.00      0.00      0.00        47\n",
      "       0,0,0       0.00      0.00      0.00         2\n",
      "      0,0,11       0.00      0.00      0.00         2\n",
      "      0,0,13       0.00      0.00      0.00         1\n",
      "       0,0,2       0.00      0.00      0.00         1\n",
      "       0,0,3       0.00      0.00      0.00         2\n",
      "       0,0,5       0.00      0.00      0.00         1\n",
      "       0,0,8       0.00      0.00      0.00         2\n",
      "         0,1       0.00      0.00      0.00        21\n",
      "       0,1,1       0.00      0.00      0.00         1\n",
      "    0,1,2,11       0.00      0.00      0.00         1\n",
      "       0,1,8       0.00      0.00      0.00         1\n",
      "        0,10       0.00      0.00      0.00        18\n",
      "     0,10,11       0.00      0.00      0.00         1\n",
      "   0,10,4,11       0.00      0.00      0.00         1\n",
      "    0,10,4,8       0.00      0.00      0.00         1\n",
      "      0,10,8       0.00      0.00      0.00         1\n",
      "        0,11       0.00      0.00      0.00        60\n",
      "      0,11,0       0.00      0.00      0.00         1\n",
      "     0,11,12       0.00      0.00      0.00         1\n",
      "      0,11,2       0.00      0.00      0.00         1\n",
      "    0,11,2,3       0.00      0.00      0.00         1\n",
      "      0,11,3       0.00      0.00      0.00         1\n",
      "        0,12       0.00      0.00      0.00         5\n",
      "        0,13       0.00      0.00      0.00        47\n",
      "         0,2       0.00      0.00      0.00        58\n",
      "       0,2,3       0.00      0.00      0.00         2\n",
      "         0,3       0.00      0.00      0.00        44\n",
      "      0,3,10       0.00      0.00      0.00         1\n",
      "      0,3,12       0.00      0.00      0.00         1\n",
      "         0,4       0.00      0.00      0.00        30\n",
      "      0,4,11       0.00      0.00      0.00         1\n",
      "       0,4,2       0.00      0.00      0.00         2\n",
      "       0,4,3       0.00      0.00      0.00         1\n",
      "       0,4,8       0.00      0.00      0.00         1\n",
      "         0,5       0.00      0.00      0.00         4\n",
      "       0,5,6       0.00      0.00      0.00         1\n",
      "         0,6       0.00      0.00      0.00         9\n",
      "       0,6,0       0.00      0.00      0.00         1\n",
      "       0,6,2       0.00      0.00      0.00         1\n",
      "       0,6,3       0.00      0.00      0.00         3\n",
      "       0,6,7       0.00      0.00      0.00         1\n",
      "       0,6,8       0.00      0.00      0.00         1\n",
      "     0,6,8,3       0.00      0.00      0.00         1\n",
      "         0,7       0.00      0.00      0.00         2\n",
      "         0,8       0.00      0.00      0.00       143\n",
      "       0,8,0       0.00      0.00      0.00         2\n",
      "      0,8,10       0.00      0.00      0.00         1\n",
      "      0,8,11       0.00      0.00      0.00         9\n",
      "       0,8,2       0.00      0.00      0.00         2\n",
      "       0,8,3       0.00      0.00      0.00         7\n",
      "       0,8,5       0.00      0.00      0.00         2\n",
      "       0,8,7       0.00      0.00      0.00         1\n",
      "       0,8,8       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00       650\n",
      "         1,0       0.00      0.00      0.00         2\n",
      "         1,1       0.00      0.00      0.00        59\n",
      "       1,1,5       0.00      0.00      0.00         3\n",
      "       1,1,6       0.00      0.00      0.00         3\n",
      "        1,10       0.00      0.00      0.00        12\n",
      "      1,10,6       0.00      0.00      0.00         1\n",
      "      1,10,7       0.00      0.00      0.00         1\n",
      "        1,11       0.00      0.00      0.00         2\n",
      "        1,13       0.00      0.00      0.00        55\n",
      "         1,2       0.00      0.00      0.00         5\n",
      "      1,2,13       0.00      0.00      0.00         1\n",
      "         1,3       0.00      0.00      0.00        18\n",
      "         1,4       0.00      0.00      0.00        14\n",
      "       1,4,3       0.00      0.00      0.00         1\n",
      "       1,4,6       0.00      0.00      0.00         1\n",
      "         1,5       0.00      0.00      0.00        29\n",
      "      1,5,10       0.00      0.00      0.00         1\n",
      "       1,5,6       0.00      0.00      0.00         6\n",
      "         1,6       0.00      0.00      0.00        70\n",
      "      1,6,10       0.00      0.00      0.00         1\n",
      "       1,6,6       0.00      0.00      0.00         5\n",
      "       1,6,7       0.00      0.00      0.00         1\n",
      "         1,7       0.00      0.00      0.00         4\n",
      "         1,8       0.00      0.00      0.00        18\n",
      "       1,8,3       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00       472\n",
      "        10,0       0.00      0.00      0.00         7\n",
      "       10,10       0.00      0.00      0.00        11\n",
      "       10,13       0.00      0.00      0.00        30\n",
      "     10,2,10       0.00      0.00      0.00         1\n",
      "        10,3       0.00      0.00      0.00         5\n",
      "        10,4       0.00      0.00      0.00        35\n",
      "      10,4,0       0.00      0.00      0.00         1\n",
      "     10,4,13       0.00      0.00      0.00         4\n",
      "      10,4,3       0.00      0.00      0.00         2\n",
      "      10,4,5       0.00      0.00      0.00         1\n",
      "        10,5       0.00      0.00      0.00         3\n",
      "        10,6       0.00      0.00      0.00         3\n",
      "     10,6,10       0.00      0.00      0.00         1\n",
      "        10,7       0.00      0.00      0.00         1\n",
      "      10,7,0       0.00      0.00      0.00         1\n",
      "        10,8       0.00      0.00      0.00         2\n",
      "      10,8,3       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00       207\n",
      "        11,0       0.00      0.00      0.00         1\n",
      "       11,10       0.00      0.00      0.00         3\n",
      "       11,12       0.00      0.00      0.00         2\n",
      "       11,13       0.00      0.00      0.00        10\n",
      "        11,2       0.00      0.00      0.00        16\n",
      "        11,3       0.00      0.00      0.00         9\n",
      "          12       0.00      0.00      0.00        20\n",
      "       12,13       0.00      0.00      0.00         1\n",
      "          13       0.32      0.98      0.48      3175\n",
      "           2       0.59      0.39      0.47       547\n",
      "         2,0       0.00      0.00      0.00         2\n",
      "        2,10       0.00      0.00      0.00         6\n",
      "        2,11       0.00      0.00      0.00         6\n",
      "        2,13       0.00      0.00      0.00        21\n",
      "         2,2       0.00      0.00      0.00         4\n",
      "         2,3       0.00      0.00      0.00        27\n",
      "      2,3,10       0.00      0.00      0.00         1\n",
      "       2,3,3       0.00      0.00      0.00         1\n",
      "         2,4       0.00      0.00      0.00         1\n",
      "       2,4,3       0.00      0.00      0.00         1\n",
      "         2,5       0.00      0.00      0.00         2\n",
      "      2,5,10       0.00      0.00      0.00         1\n",
      "       2,5,3       0.00      0.00      0.00         1\n",
      "         2,6       0.00      0.00      0.00         2\n",
      "      2,6,11       0.00      0.00      0.00         1\n",
      "         2,7       0.00      0.00      0.00         3\n",
      "         2,8       0.00      0.00      0.00         1\n",
      "       2,8,3       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00       349\n",
      "         3,0       0.00      0.00      0.00        11\n",
      "        3,10       0.00      0.00      0.00        11\n",
      "        3,12       0.00      0.00      0.00         1\n",
      "        3,13       0.00      0.00      0.00        42\n",
      "         3,3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00       427\n",
      "         4,0       0.00      0.00      0.00        18\n",
      "      4,0,11       0.00      0.00      0.00         1\n",
      "       4,0,3       0.00      0.00      0.00         2\n",
      "        4,10       0.00      0.00      0.00         9\n",
      "        4,11       0.00      0.00      0.00         3\n",
      "      4,11,2       0.00      0.00      0.00         1\n",
      "        4,13       0.00      0.00      0.00        29\n",
      "         4,2       0.00      0.00      0.00        12\n",
      "      4,2,10       0.00      0.00      0.00         1\n",
      "         4,3       0.00      0.00      0.00        25\n",
      "         4,4       0.00      0.00      0.00         4\n",
      "       4,4,0       0.00      0.00      0.00         1\n",
      "         4,5       0.00      0.00      0.00         4\n",
      "      4,5,10       0.00      0.00      0.00         2\n",
      "   4,5,10,10       0.00      0.00      0.00         1\n",
      "         4,6       0.00      0.00      0.00         9\n",
      "      4,6,10       0.00      0.00      0.00         3\n",
      "       4,6,2       0.00      0.00      0.00         1\n",
      "       4,6,6       0.00      0.00      0.00         1\n",
      "      4,7,10       0.00      0.00      0.00         1\n",
      "         4,8       0.00      0.00      0.00        11\n",
      "       4,8,3       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00       176\n",
      "         5,0       0.00      0.00      0.00         2\n",
      "        5,10       0.00      0.00      0.00        26\n",
      "     5,10,10       0.00      0.00      0.00         1\n",
      "     5,10,13       0.00      0.00      0.00         1\n",
      "        5,13       0.00      0.00      0.00        24\n",
      "         5,2       0.00      0.00      0.00         2\n",
      "         5,3       0.00      0.00      0.00         8\n",
      "         5,6       0.00      0.00      0.00        15\n",
      "         5,7       0.00      0.00      0.00         3\n",
      "         5,8       0.00      0.00      0.00         1\n",
      "         5,9       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00       544\n",
      "         6,0       0.00      0.00      0.00         2\n",
      "      6,0,11       0.00      0.00      0.00         1\n",
      "        6,10       0.00      0.00      0.00        14\n",
      "        6,13       0.00      0.00      0.00        38\n",
      "         6,2       0.00      0.00      0.00         3\n",
      "         6,3       0.00      0.00      0.00        21\n",
      "      6,3,10       0.00      0.00      0.00         2\n",
      "      6,3,12       0.00      0.00      0.00         1\n",
      "         6,6       0.00      0.00      0.00        14\n",
      "      6,6,13       0.00      0.00      0.00         1\n",
      "         6,7       0.00      0.00      0.00         1\n",
      "       6,7,3       0.00      0.00      0.00         1\n",
      "         6,8       0.00      0.00      0.00         3\n",
      "         6,9       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       142\n",
      "         7,0       0.00      0.00      0.00         1\n",
      "        7,10       0.00      0.00      0.00         6\n",
      "        7,13       0.00      0.00      0.00         7\n",
      "         7,2       0.00      0.00      0.00         1\n",
      "         7,3       0.00      0.00      0.00         1\n",
      "      7,3,10       0.00      0.00      0.00         1\n",
      "         7,7       0.00      0.00      0.00         6\n",
      "      7,8,10       0.00      0.00      0.00         1\n",
      "           8       0.69      0.22      0.34       980\n",
      "         8,0       0.00      0.00      0.00        14\n",
      "       8,0,3       0.00      0.00      0.00         1\n",
      "        8,10       0.00      0.00      0.00        16\n",
      "        8,11       0.00      0.00      0.00        14\n",
      "      8,11,3       0.00      0.00      0.00         1\n",
      "        8,12       0.00      0.00      0.00         4\n",
      "        8,13       0.00      0.00      0.00        45\n",
      "         8,2       0.00      0.00      0.00        48\n",
      "       8,2,3       0.00      0.00      0.00         1\n",
      "         8,3       0.00      0.00      0.00        54\n",
      "      8,3,12       0.00      0.00      0.00         1\n",
      "      8,3,13       0.00      0.00      0.00         1\n",
      "       8,3,3       0.00      0.00      0.00         1\n",
      "         8,4       0.00      0.00      0.00        11\n",
      "         8,5       0.00      0.00      0.00         2\n",
      "         8,6       0.00      0.00      0.00         8\n",
      "         8,7       0.00      0.00      0.00         1\n",
      "         8,8       0.00      0.00      0.00        19\n",
      "      8,9,10       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00        14\n",
      "         9,0       0.00      0.00      0.00         1\n",
      "        9,10       0.00      0.00      0.00         6\n",
      "     9,10,10       0.00      0.00      0.00         1\n",
      "        9,13       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.34     10853\n",
      "   macro avg       0.01      0.01      0.01     10853\n",
      "weighted avg       0.22      0.34      0.21     10853\n",
      "\n",
      "Accuracy: 0.33603611904542524\n",
      "Precision: 0.33603611904542524\n",
      "Recall: 0.33603611904542524\n",
      "F1 score: 0.33603611904542524\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\\n\", report)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 4 - train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y=df['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_train, X_test, y_train, y_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the total number of Training Data : (48836,)\n",
      "\n",
      " the total number of Test Data : (5427,)\n",
      "Accuracy: 0.5034088815183343\n",
      "\n",
      " the total number of Training Data : (43410,)\n",
      "\n",
      " the total number of Test Data : (10853,)\n",
      "Accuracy: 0.4945176448908136\n",
      "\n",
      " the total number of Training Data : (37984,)\n",
      "\n",
      " the total number of Test Data : (16279,)\n",
      "Accuracy: 0.49431783279071195\n"
     ]
    }
   ],
   "source": [
    "per = [0.1, 0.2, 0.3]\n",
    "for i in per:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=i, random_state=42)\n",
    "    print ('\\nThe total number of Training Data :',y_train.shape)\n",
    "    print ('The total number of Test Data :',y_test.shape)\n",
    "    test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlwFGj1mYe5Xhnj3v/j9Wn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
